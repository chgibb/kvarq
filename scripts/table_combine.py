
import csv
import argparse
import os.path
import json
import codecs
import sys
import bisect

from table_utils import csv_xls_wrapper, csv_xls_writer
from kvarq.analyse import AnalyserJson

parser = argparse.ArgumentParser(
        description='combines results from a set of .json files (such as '
        'generated by table_scan.py) with a .csv/.xls table')

parser.add_argument('-d', '--debug', action='store_true',
        help='show debug information on console')

parser.add_argument('-c', '--column', type=int, default=0,
        help='specify column containing fastq name (defaults to 0=first column)')
parser.add_argument('-i', '--insert', type=int, default=1,
        help='specify column where results should be inserted (defaults to 1=second column)')

parser.add_argument('table',
        help='.csv/.xls input table')
parser.add_argument('directory',
        help='directory containing .json files')

args = parser.parse_args()

def cumsum(l):
    sum = 0
    for x in l:
        sum += x
        yield sum

converters = {
        'str': str, # default converter
        'int': int,
        'MB': lambda x: '%.2f'%(x/1e6),
        'median': lambda x: bisect.bisect_left(list(cumsum(x)), sum(x)/2),
        'config': lambda x: '-Q %s -o %d'% ('Azero' in x and str(ord(x['Amin'])-ord(x['Azero'])) or '?', x['minoverlap'])
    }

export_columns = (
        {
            'path' : ('analyses', 'phylo'),
            'heading' : 'kvarq: phylo'
        },
        {
            'path' : ('analyses', 'resistance'),
            'heading' : 'kvarq: resistance'
        },
        {
            'path' : ('info', 'when'),
            'heading' : 'when',
        },
        {
            'path' : ('info', 'config'),
            'heading' : 'config',
            'converter' : 'config'
        },
        {
            'path' : ('info', 'scantime'),
            'heading' : 'scantime',
            'converter' : 'int'
        },
        {
            'path' : ('info', 'size'),
            'heading' : 'filesize',
            'converter' : 'MB'
        },
        {
            'path' : ('info', 'readlength'),
            'heading' : '~readlength'
        },
        {
            'path' : ('stats', 'readlengths'),
            'heading' : 'median readlength',
            'converter' : 'median'
        },
        {
            'path' : ('stats', 'records_parsed'),
            'heading' : 'records parsed'
        },
    )

table = csv_xls_wrapper(args.table)
outfn = os.path.join(args.directory, os.path.basename(args.table))
out = csv_xls_writer(outfn, autoflush=False)
new_columns = 0
header = None
analyses = None
fastqidx = args.column

for row in table:

    if not header:
        header = row
        for ec in export_columns:
            if ec['heading'] in header:
                if args.debug:
                    print 'column "%s" already found in header'% ec['heading']
                continue
            header.insert(args.insert + new_columns, ec['heading'])
            new_columns += 1

        if args.insert <= args.column:
            fastqidx += args.insert

        if args.debug:
            print 'inserted %d new columns at %d..%d'% (new_columns, args.insert, args.insert+new_columns-1)

        out.writerow(header)
        continue

    for i in range(new_columns):
        row.insert(args.insert, '')

    jsonfn = os.path.basename(row[fastqidx])
    jsonfn = os.path.splitext(jsonfn)[0] + '.json'
    jsonfn = os.path.join(args.directory, jsonfn)

    if os.path.exists(jsonfn):
        try:
            data = AnalyserJson(jsonfn)
        except Exception, e:
            print '*** invalid format "%s" : %s'% (jsonfn, str(e))
            out.writerow(row, {fastqidx : 3})

        if args.debug:
            print 'processing ' + jsonfn

        for ec in export_columns:
            value = data.data
            path = list(ec['path'])
            while path:
                if path[0] not in value:
                    break
                value = value[path[0]]
                del path[0]

            if path:
                value = None
            else:
                value = converters[ec.get('converter', 'str')](value)

            row[header.index(ec['heading'])] = value

        out.writerow(row)

    else:
        if args.debug:
            print 'skipping (not found) : "%s"'% jsonfn
        out.writerow(row)


out.flush()

